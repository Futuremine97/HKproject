{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "129f4edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52,54,58,62,63,66,68,70,68,70,70,73,74,75,77,78,Mei\n",
      "65,68,66,68,71,74,74,76,72,74,74,75,76,77,80,80,조진호\n",
      "51,55,58,60,66,67,67,69,68,70,74,73,78,76,78,80,최주원\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "\n",
    "with open(\"mei.csv\", 'r', encoding=\"utf-8\") as read_: #read다음에_는 의미가 없다. read랑 헷갈리지말라고 쓴 것. a 라고 해도 무방.\n",
    "    text=read_.readlines()\n",
    "\n",
    "rows=[]\n",
    "for i in text[1:]:\n",
    "    rows.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "df=pd.DataFrame(rows)\n",
    "df.columns=[\"sensor%d\"%i for i in range(1,17)]\n",
    "\n",
    "with open(\"jo.csv\", 'r', encoding=\"utf-8\") as read_:\n",
    "    text2=read_.readlines()\n",
    "    \n",
    "\n",
    "rows2=[]\n",
    "for i in text2[1:]:\n",
    "    rows2.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "with open(\"joo.csv\", 'r', encoding=\"utf-8\") as read_:\n",
    "    text3=read_.readlines()\n",
    "    \n",
    "rows3=[]\n",
    "for i in text3[1:]:\n",
    "    rows3.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "df2=pd.DataFrame(rows2)\n",
    "df2.columns=[\"sensor%d\"%i for i in range(1,17)]\n",
    "\n",
    "\n",
    "df3=pd.DataFrame(rows3)\n",
    "df3.columns=[\"sensor%d\"%i for i in range(1,17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71d032fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16)\n",
      "   sensor1  sensor2  sensor3  sensor4  sensor5  sensor6  sensor7  sensor8  \\\n",
      "0       91       93       85       90       95       93       86       91   \n",
      "1       70       73       69       71       76       76       77       78   \n",
      "2       65       64       68       67       72       71       74       73   \n",
      "3       13       19       24       29       35       37       41       47   \n",
      "4      102      100      102       97       99       99       97       96   \n",
      "\n",
      "   sensor9  sensor10  sensor11  sensor12  sensor13  sensor14  sensor15  \\\n",
      "0       87        87        87        86        88        87        90   \n",
      "1       74        75        76        78        79        80        78   \n",
      "2       73        70        72        74        77        77        78   \n",
      "3       44        48        51        55        58        59        59   \n",
      "4       92        91        89        89        91        91        90   \n",
      "\n",
      "   sensor16  \n",
      "0        87  \n",
      "1        81  \n",
      "2        78  \n",
      "3        62  \n",
      "4        88  \n",
      "(20, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import seaborn;\n",
    "seaborn.set()\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "image1 = df.astype('float')\n",
    "image2 = df2.astype('float')\n",
    "\n",
    "n_channel_1=16\n",
    "n_channel_2=32\n",
    "n_dense=32\n",
    "n_train_epoch=10\n",
    "\n",
    "y2 = ['sensor1']\n",
    "y1 = ['sensor1','sensor2','sensor3','sensor4','sensor5','sensor6','sensor7','sensor8','sensor9','sensor10','sensor11','sensor12','sensor13','sensor14','sensor15','sensor16'] # define y variable, i.e., what we want to predict\n",
    "print(df.shape) # print the number of rows anc columns\n",
    "\n",
    "print(df.head())\n",
    "print(df2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "145f3878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96, 94, 92, 92, 93, 93, 95, 92, 90, 89, 89, 88, 89, 92, 91, 90], [92, 92, 92, 91, 93, 94, 91, 95, 87, 88, 86, 86, 88, 90, 87, 88], [112, 110, 107, 106, 107, 104, 102, 102, 99, 96, 96, 95, 88, 94, 97, 93], [93, 91, 90, 91, 93, 92, 91, 91, 87, 87, 88, 89, 87, 87, 87, 88], [93, 93, 90, 91, 92, 93, 93, 91, 87, 87, 87, 87, 90, 88, 89, 88], [92, 93, 92, 90, 94, 92, 91, 92, 87, 88, 84, 86, 87, 87, 85, 88], [91, 91, 88, 91, 89, 92, 89, 89, 85, 86, 85, 87, 87, 86, 85, 86], [89, 89, 89, 88, 90, 91, 88, 87, 86, 85, 84, 83, 86, 87, 87, 85], [89, 87, 86, 86, 87, 90, 87, 89, 82, 86, 83, 84, 84, 79, 84, 85], [91, 90, 90, 90, 94, 90, 88, 89, 87, 85, 85, 86, 86, 87, 85, 89], [95, 94, 92, 92, 94, 92, 91, 92, 88, 87, 87, 86, 89, 87, 85, 87], [69, 69, 73, 73, 75, 76, 76, 76, 73, 74, 74, 76, 78, 79, 79, 81], [92, 91, 91, 90, 91, 90, 88, 89, 83, 85, 85, 85, 84, 85, 85, 84], [77, 77, 77, 80, 78, 79, 79, 81, 77, 78, 77, 79, 79, 80, 79, 81], [74, 74, 76, 75, 77, 76, 78, 77, 79, 76, 76, 81, 79, 74, 79, 80], [74, 74, 75, 79, 76, 81, 78, 80, 76, 75, 79, 75, 82, 79, 74, 79]]\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "[[91, 93, 85, 90, 95, 93, 86, 91, 87, 87, 87, 86, 88, 87, 90, 87], [70, 73, 69, 71, 76, 76, 77, 78, 74, 75, 76, 78, 79, 80, 78, 81], [65, 64, 68, 67, 72, 71, 74, 73, 73, 70, 72, 74, 77, 77, 78, 78], [13, 19, 24, 29, 35, 37, 41, 47, 44, 48, 51, 55, 58, 59, 59, 62], [102, 100, 102, 97, 99, 99, 97, 96, 92, 91, 89, 89, 91, 91, 90, 88], [76, 78, 76, 77, 79, 80, 81, 81, 79, 81, 80, 81, 81, 83, 82, 80], [70, 71, 72, 72, 76, 76, 77, 77, 74, 74, 71, 76, 81, 79, 80, 80], [167, 163, 157, 151, 149, 145, 139, 136, 130, 127, 122, 120, 120, 120, 115, 117], [36, 40, 44, 47, 50, 53, 58, 57, 57, 59, 61, 63, 67, 69, 71, 69], [50, 56, 55, 57, 62, 64, 64, 67, 64, 67, 68, 65, 71, 75, 73, 75], [50, 52, 52, 56, 62, 64, 65, 66, 65, 66, 69, 68, 73, 73, 73, 74], [43, 49, 50, 54, 57, 58, 61, 64, 61, 65, 67, 66, 69, 70, 71, 74], [79, 75, 73, 77, 80, 81, 76, 82, 80, 79, 81, 82, 85, 84, 84, 83], [79, 79, 79, 81, 82, 82, 83, 85, 81, 81, 81, 83, 86, 87, 87, 85], [88, 88, 90, 92, 91, 91, 89, 92, 87, 88, 87, 88, 89, 90, 89, 90], [63, 63, 68, 68, 70, 71, 76, 75, 74, 74, 75, 75, 78, 78, 79, 80]]\n",
      "------------------------------------------------------------------------------------------\n",
      "[[74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [95 94 92 92 94 92 91 92 88 87 87 86 89 87 85 87]]\n",
      "------------------------------------------------------------------------------------------\n",
      "[[74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      " [95 94 92 92 94 92 91 92 88 87 87 86 89 87 85 87]]\n",
      "------------------------------------------------------------------------------------------\n",
      "[[60, 62, 63, 66, 67, 69, 72, 76, 71, 73, 74, 79, 78, 79, 79, 83], [60, 62, 67, 66, 73, 69, 73, 72, 74, 73, 76, 75, 74, 79, 80, 80], [59, 60, 64, 66, 70, 70, 71, 73, 71, 73, 72, 75, 77, 82, 78, 74], [52, 54, 58, 62, 63, 66, 68, 70, 68, 70, 70, 73, 74, 75, 77, 78]]\n",
      "------------------------------------------------------------------------------------------\n",
      "[[60, 62, 63, 66, 67, 69, 72, 76, 71, 73, 74, 79, 78, 79, 79, 83], [60, 62, 67, 66, 73, 69, 73, 72, 74, 73, 76, 75, 74, 79, 80, 80], [59, 60, 64, 66, 70, 70, 71, 73, 71, 73, 72, 75, 77, 82, 78, 74], [52, 54, 58, 62, 63, 66, 68, 70, 68, 70, 70, 73, 74, 75, 77, 78]]\n",
      "[[[74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      "  [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      "  [74 74 75 79 76 81 78 80 76 75 79 75 82 79 74 79]\n",
      "  [95 94 92 92 94 92 91 92 88 87 87 86 89 87 85 87]]]\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_67/12416806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"정답률=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m# from sklearn.datasets import make_blobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             raise ValueError(\n\u001b[1;32m    497\u001b[0m                 \u001b[0;34m\"y must have at least two dimensions for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# 데이터셋 로드하기\n",
    "# [[your code]\n",
    "data = rows\n",
    "data2 = rows2\n",
    "# 훈련용 데이터셋 나누기\n",
    "# [[your code]\n",
    "X_train = rows[:16]\n",
    "X_test = rows[16:]\n",
    "\n",
    "y_train = data2[:16]\n",
    "y_test = data[16:]\n",
    "\n",
    "print(y_train)\n",
    "print('-'*90)\n",
    "print('-'*90)\n",
    "print(X_train)\n",
    "# y_test2 = data2[:16]\n",
    "# y_test2 = y_test2.reshape(16,4)\n",
    "# 훈련하기\n",
    "# [[your code]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print('-'*90)\n",
    "# 예측하기\n",
    "# [[your code]\n",
    "y_pred1 = model.predict(X_test)\n",
    "# 정답률 출력하기\n",
    "# [[your code]\n",
    "print(y_pred1)\n",
    "print('-'*90)\n",
    "decision_tree = DecisionTreeClassifier(random_state=32)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred2 = decision_tree.predict(X_test)\n",
    "print(y_pred2)\n",
    "print('-'*90)\n",
    "\n",
    "y_testt = np.transpose(y_test)\n",
    "print(y_test)\n",
    "print('-'*90)\n",
    "y_pred1 = np.expand_dims(y_pred1, axis=0)\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "print(y_pred1)\n",
    "print('-'*90)\n",
    "print('-'*90)\n",
    "rows4=[]\n",
    "for i in y_pred1[1:]:\n",
    "    rows4.append(list(map(int,i.split(\",\"))))\n",
    "\n",
    "print(rows4)\n",
    "\n",
    "print(\"정답률=\", clf.score(y_pred1, y_test))\n",
    "# from sklearn.datasets import make_blobs\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# X,y = make_multilabel_classification(n_classes= 3,random_state = 0)\n",
    "# clf = MultiOutputClassifier(KNeighborsClassifier()).fit(X,y_train)\n",
    "# clf.predict(X[-2:])\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# classifier = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "# classifier.fit(y_test,y_pred)\n",
    "# predictions = classifier.predict(y_train)\n",
    "# classifier.score(y_true,np.array(y_train))\n",
    "\n",
    "# from sklearn.linear_model import RidgeClassifierCV\n",
    "# clf = RidgeClassifierCV().fit(X_train, y_train)\n",
    "# y_score = clf.decision_function(X_train)\n",
    "# roc_auc_score(y_test, y_train, average=None)\n",
    "\n",
    "# from sklearn.datasets import make_multilabel_classification\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# X, y = make_multilabel_classification(random_state=0)\n",
    "# inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "# clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "# y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "# roc_auc_score(y, y_score, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9cf03",
   "metadata": {},
   "source": [
    "\n",
    "#열이 16개인 데이터셋\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32882f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
