{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29a0c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52,54,58,62,63,66,68,70,68,70,70,73,74,75,77,78,Mei\n",
      "65,68,66,68,71,74,74,76,72,74,74,75,76,77,80,80,조진호\n",
      "51,55,58,60,66,67,67,69,68,70,74,73,78,76,78,80,최주원\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "\n",
    "with open(\"mei.csv\", 'r', encoding=\"utf-8\") as read_: #read다음에_는 의미가 없다. read랑 헷갈리지말라고 쓴 것. a 라고 해도 무방.\n",
    "    text=read_.readlines()\n",
    "\n",
    "rows=[]\n",
    "for i in text[1:]:\n",
    "    rows.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "df=pd.DataFrame(rows)\n",
    "df.columns=[\"sensor%d\"%i for i in range(1,17)]\n",
    "\n",
    "with open(\"jo.csv\", 'r', encoding=\"utf-8\") as read_:\n",
    "    text2=read_.readlines()\n",
    "    \n",
    "\n",
    "rows2=[]\n",
    "for i in text2[1:]:\n",
    "    rows2.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "with open(\"joo.csv\", 'r', encoding=\"utf-8\") as read_:\n",
    "    text3=read_.readlines()\n",
    "    \n",
    "rows9=[]\n",
    "for i in text3[1:]:\n",
    "    rows9.append(list(map(int,i.split(\",\")[:-1])))\n",
    "print(i)\n",
    "\n",
    "df2=pd.DataFrame(rows2)\n",
    "df2.columns=[\"sensor%d\"%i for i in range(1,17)]\n",
    "\n",
    "\n",
    "df3=pd.DataFrame(rows3)\n",
    "df3.columns=[\"sensor%d\"%i for i in range(1,17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72353720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16)\n",
      "   sensor1  sensor2  sensor3  sensor4  sensor5  sensor6  sensor7  sensor8  \\\n",
      "0       91       93       85       90       95       93       86       91   \n",
      "1       70       73       69       71       76       76       77       78   \n",
      "2       65       64       68       67       72       71       74       73   \n",
      "3       13       19       24       29       35       37       41       47   \n",
      "4      102      100      102       97       99       99       97       96   \n",
      "\n",
      "   sensor9  sensor10  sensor11  sensor12  sensor13  sensor14  sensor15  \\\n",
      "0       87        87        87        86        88        87        90   \n",
      "1       74        75        76        78        79        80        78   \n",
      "2       73        70        72        74        77        77        78   \n",
      "3       44        48        51        55        58        59        59   \n",
      "4       92        91        89        89        91        91        90   \n",
      "\n",
      "   sensor16  \n",
      "0        87  \n",
      "1        81  \n",
      "2        78  \n",
      "3        62  \n",
      "4        88  \n",
      "(20, 16)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import seaborn;\n",
    "seaborn.set()\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "image1 = df.astype('float')\n",
    "image2 = df2.astype('float')\n",
    "\n",
    "n_channel_1=16\n",
    "n_channel_2=32\n",
    "n_dense=32\n",
    "n_train_epoch=10\n",
    "\n",
    "y2 = ['sensor1']\n",
    "y1 = ['sensor1','sensor2','sensor3','sensor4','sensor5','sensor6','sensor7','sensor8','sensor9','sensor10','sensor11','sensor12','sensor13','sensor14','sensor15','sensor16'] # define y variable, i.e., what we want to predict\n",
    "print(df.shape) # print the number of rows anc columns\n",
    "\n",
    "print(df.head())\n",
    "print(df2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "97bf3adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train\n",
      "[[91, 93, 85, 90, 95, 93, 86, 91, 87, 87, 87, 86, 88, 87, 90, 87], [70, 73, 69, 71, 76, 76, 77, 78, 74, 75, 76, 78, 79, 80, 78, 81], [65, 64, 68, 67, 72, 71, 74, 73, 73, 70, 72, 74, 77, 77, 78, 78], [13, 19, 24, 29, 35, 37, 41, 47, 44, 48, 51, 55, 58, 59, 59, 62], [102, 100, 102, 97, 99, 99, 97, 96, 92, 91, 89, 89, 91, 91, 90, 88], [76, 78, 76, 77, 79, 80, 81, 81, 79, 81, 80, 81, 81, 83, 82, 80], [70, 71, 72, 72, 76, 76, 77, 77, 74, 74, 71, 76, 81, 79, 80, 80], [167, 163, 157, 151, 149, 145, 139, 136, 130, 127, 122, 120, 120, 120, 115, 117], [36, 40, 44, 47, 50, 53, 58, 57, 57, 59, 61, 63, 67, 69, 71, 69], [50, 56, 55, 57, 62, 64, 64, 67, 64, 67, 68, 65, 71, 75, 73, 75], [50, 52, 52, 56, 62, 64, 65, 66, 65, 66, 69, 68, 73, 73, 73, 74], [43, 49, 50, 54, 57, 58, 61, 64, 61, 65, 67, 66, 69, 70, 71, 74], [79, 75, 73, 77, 80, 81, 76, 82, 80, 79, 81, 82, 85, 84, 84, 83], [79, 79, 79, 81, 82, 82, 83, 85, 81, 81, 81, 83, 86, 87, 87, 85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "X_train\n",
      "[[91, 93, 85, 90, 95, 93, 86, 91, 87, 87, 87, 86, 88, 87, 90, 87], [70, 73, 69, 71, 76, 76, 77, 78, 74, 75, 76, 78, 79, 80, 78, 81], [65, 64, 68, 67, 72, 71, 74, 73, 73, 70, 72, 74, 77, 77, 78, 78], [13, 19, 24, 29, 35, 37, 41, 47, 44, 48, 51, 55, 58, 59, 59, 62], [102, 100, 102, 97, 99, 99, 97, 96, 92, 91, 89, 89, 91, 91, 90, 88], [76, 78, 76, 77, 79, 80, 81, 81, 79, 81, 80, 81, 81, 83, 82, 80], [70, 71, 72, 72, 76, 76, 77, 77, 74, 74, 71, 76, 81, 79, 80, 80], [167, 163, 157, 151, 149, 145, 139, 136, 130, 127, 122, 120, 120, 120, 115, 117], [36, 40, 44, 47, 50, 53, 58, 57, 57, 59, 61, 63, 67, 69, 71, 69], [50, 56, 55, 57, 62, 64, 64, 67, 64, 67, 68, 65, 71, 75, 73, 75], [50, 52, 52, 56, 62, 64, 65, 66, 65, 66, 69, 68, 73, 73, 73, 74], [43, 49, 50, 54, 57, 58, 61, 64, 61, 65, 67, 66, 69, 70, 71, 74], [79, 75, 73, 77, 80, 81, 76, 82, 80, 79, 81, 82, 85, 84, 84, 83], [79, 79, 79, 81, 82, 82, 83, 85, 81, 81, 81, 83, 86, 87, 87, 85]]\n",
      "-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/\n",
      "-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/\n",
      "xt : X_train 에 해당\n",
      "[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "  69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "  72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "  41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "  92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "  80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "  81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      " 115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "  50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "  52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "  57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "  76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "  81  81  81  83  86  87  87  85]\n",
      "------------------------------------------------------------------------------------------\n",
      "yt: y_train 에 해당 \n",
      "[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "  69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "  72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "  41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "  92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "  80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "  81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      " 115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "  50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "  52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "  57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "  76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "  81  81  81  83  86  87  87  85]\n",
      "------------------------------------------------------------------------------------------\n",
      "ett : y_test 에 해당\n",
      "[88 88 90 92 91 91 89 92 87 88 87 88 89 90 89 90 63 63 68 68 70 71 76 75\n",
      " 74 74 75 75 78 78 79 80 60 62 63 66 67 69 72 76 71 73 74 79 78 79 79 83\n",
      " 60 62 67 66 73 69 73 72 74 73 76 75 74 79 80 80 59 60 64 66 70 70 71 73\n",
      " 71 73 72 75 77 82 78 74 52 54 58 62 63 66 68 70 68 70 70 73 74 75 77 78]\n",
      "------------------------------------------------------------------------------------------\n",
      "xt\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "yt\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "knn 예측치\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "정답률= 1.0\n",
      "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
      "------------------------------------------------------------------------------------------\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "Decision Tree classifier 예측치\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "[[88, 88, 90, 92, 91, 91, 89, 92, 87, 88, 87, 88, 89, 90, 89, 90], [63, 63, 68, 68, 70, 71, 76, 75, 74, 74, 75, 75, 78, 78, 79, 80], [60, 62, 63, 66, 67, 69, 72, 76, 71, 73, 74, 79, 78, 79, 79, 83], [60, 62, 67, 66, 73, 69, 73, 72, 74, 73, 76, 75, 74, 79, 80, 80], [59, 60, 64, 66, 70, 70, 71, 73, 71, 73, 72, 75, 77, 82, 78, 74], [52, 54, 58, 62, 63, 66, 68, 70, 68, 70, 70, 73, 74, 75, 77, 78]]\n",
      "------------------------------------------------------------------------------------------\n",
      "y_test\n",
      "[[88, 88, 90, 92, 91, 91, 89, 92, 87, 88, 87, 88, 89, 90, 89, 90], [63, 63, 68, 68, 70, 71, 76, 75, 74, 74, 75, 75, 78, 78, 79, 80], [60, 62, 63, 66, 67, 69, 72, 76, 71, 73, 74, 79, 78, 79, 79, 83], [60, 62, 67, 66, 73, 69, 73, 72, 74, 73, 76, 75, 74, 79, 80, 80], [59, 60, 64, 66, 70, 70, 71, 73, 71, 73, 72, 75, 77, 82, 78, 74], [52, 54, 58, 62, 63, 66, 68, 70, 68, 70, 70, 73, 74, 75, 77, 78]]\n",
      "------------------------------------------------------------------------------------------\n",
      "y_pred1\n",
      "[[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "   69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "   72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "   41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "   92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "   80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "   81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      "  115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "   50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "   52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "   57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "   76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "   81  81  81  83  86  87  87  85]]\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  214  215  216  217  \\\n",
      "0   58   59   60   63   66   69   70   70   70   70  ...   50   48   52   55   \n",
      "\n",
      "   218  219  220  221  222  223  \n",
      "0   59   58   62   64   66   68  \n",
      "\n",
      "[1 rows x 224 columns]\n",
      "[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "  69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "  72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "  41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "  92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "  80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "  81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      " 115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "  50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "  52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "  57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "  76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "  81  81  81  83  86  87  87  85]\n",
      ",=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=,=\n",
      "xt3\n",
      "[[ 91]\n",
      " [ 93]\n",
      " [ 85]\n",
      " [ 90]\n",
      " [ 95]\n",
      " [ 93]\n",
      " [ 86]\n",
      " [ 91]\n",
      " [ 87]\n",
      " [ 87]\n",
      " [ 87]\n",
      " [ 86]\n",
      " [ 88]\n",
      " [ 87]\n",
      " [ 90]\n",
      " [ 87]\n",
      " [ 70]\n",
      " [ 73]\n",
      " [ 69]\n",
      " [ 71]\n",
      " [ 76]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 74]\n",
      " [ 75]\n",
      " [ 76]\n",
      " [ 78]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 78]\n",
      " [ 81]\n",
      " [ 65]\n",
      " [ 64]\n",
      " [ 68]\n",
      " [ 67]\n",
      " [ 72]\n",
      " [ 71]\n",
      " [ 74]\n",
      " [ 73]\n",
      " [ 73]\n",
      " [ 70]\n",
      " [ 72]\n",
      " [ 74]\n",
      " [ 77]\n",
      " [ 77]\n",
      " [ 78]\n",
      " [ 78]\n",
      " [ 13]\n",
      " [ 19]\n",
      " [ 24]\n",
      " [ 29]\n",
      " [ 35]\n",
      " [ 37]\n",
      " [ 41]\n",
      " [ 47]\n",
      " [ 44]\n",
      " [ 48]\n",
      " [ 51]\n",
      " [ 55]\n",
      " [ 58]\n",
      " [ 59]\n",
      " [ 59]\n",
      " [ 62]\n",
      " [102]\n",
      " [100]\n",
      " [102]\n",
      " [ 97]\n",
      " [ 99]\n",
      " [ 99]\n",
      " [ 97]\n",
      " [ 96]\n",
      " [ 92]\n",
      " [ 91]\n",
      " [ 89]\n",
      " [ 89]\n",
      " [ 91]\n",
      " [ 91]\n",
      " [ 90]\n",
      " [ 88]\n",
      " [ 76]\n",
      " [ 78]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 81]\n",
      " [ 79]\n",
      " [ 81]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 81]\n",
      " [ 83]\n",
      " [ 82]\n",
      " [ 80]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 72]\n",
      " [ 72]\n",
      " [ 76]\n",
      " [ 76]\n",
      " [ 77]\n",
      " [ 77]\n",
      " [ 74]\n",
      " [ 74]\n",
      " [ 71]\n",
      " [ 76]\n",
      " [ 81]\n",
      " [ 79]\n",
      " [ 80]\n",
      " [ 80]\n",
      " [167]\n",
      " [163]\n",
      " [157]\n",
      " [151]\n",
      " [149]\n",
      " [145]\n",
      " [139]\n",
      " [136]\n",
      " [130]\n",
      " [127]\n",
      " [122]\n",
      " [120]\n",
      " [120]\n",
      " [120]\n",
      " [115]\n",
      " [117]\n",
      " [ 36]\n",
      " [ 40]\n",
      " [ 44]\n",
      " [ 47]\n",
      " [ 50]\n",
      " [ 53]\n",
      " [ 58]\n",
      " [ 57]\n",
      " [ 57]\n",
      " [ 59]\n",
      " [ 61]\n",
      " [ 63]\n",
      " [ 67]\n",
      " [ 69]\n",
      " [ 71]\n",
      " [ 69]\n",
      " [ 50]\n",
      " [ 56]\n",
      " [ 55]\n",
      " [ 57]\n",
      " [ 62]\n",
      " [ 64]\n",
      " [ 64]\n",
      " [ 67]\n",
      " [ 64]\n",
      " [ 67]\n",
      " [ 68]\n",
      " [ 65]\n",
      " [ 71]\n",
      " [ 75]\n",
      " [ 73]\n",
      " [ 75]\n",
      " [ 50]\n",
      " [ 52]\n",
      " [ 52]\n",
      " [ 56]\n",
      " [ 62]\n",
      " [ 64]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 65]\n",
      " [ 66]\n",
      " [ 69]\n",
      " [ 68]\n",
      " [ 73]\n",
      " [ 73]\n",
      " [ 73]\n",
      " [ 74]\n",
      " [ 43]\n",
      " [ 49]\n",
      " [ 50]\n",
      " [ 54]\n",
      " [ 57]\n",
      " [ 58]\n",
      " [ 61]\n",
      " [ 64]\n",
      " [ 61]\n",
      " [ 65]\n",
      " [ 67]\n",
      " [ 66]\n",
      " [ 69]\n",
      " [ 70]\n",
      " [ 71]\n",
      " [ 74]\n",
      " [ 79]\n",
      " [ 75]\n",
      " [ 73]\n",
      " [ 77]\n",
      " [ 80]\n",
      " [ 81]\n",
      " [ 76]\n",
      " [ 82]\n",
      " [ 80]\n",
      " [ 79]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 85]\n",
      " [ 84]\n",
      " [ 84]\n",
      " [ 83]\n",
      " [ 79]\n",
      " [ 79]\n",
      " [ 79]\n",
      " [ 81]\n",
      " [ 82]\n",
      " [ 82]\n",
      " [ 83]\n",
      " [ 85]\n",
      " [ 81]\n",
      " [ 81]\n",
      " [ 81]\n",
      " [ 83]\n",
      " [ 86]\n",
      " [ 87]\n",
      " [ 87]\n",
      " [ 85]]\n",
      "/./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "[[88]\n",
      " [88]\n",
      " [90]\n",
      " [92]\n",
      " [91]\n",
      " [91]\n",
      " [89]\n",
      " [92]\n",
      " [87]\n",
      " [88]\n",
      " [87]\n",
      " [88]\n",
      " [89]\n",
      " [90]\n",
      " [89]\n",
      " [90]\n",
      " [63]\n",
      " [63]\n",
      " [68]\n",
      " [68]\n",
      " [70]\n",
      " [71]\n",
      " [76]\n",
      " [75]\n",
      " [74]\n",
      " [74]\n",
      " [75]\n",
      " [75]\n",
      " [78]\n",
      " [78]\n",
      " [79]\n",
      " [80]\n",
      " [60]\n",
      " [62]\n",
      " [63]\n",
      " [66]\n",
      " [67]\n",
      " [69]\n",
      " [72]\n",
      " [76]\n",
      " [71]\n",
      " [73]\n",
      " [74]\n",
      " [79]\n",
      " [78]\n",
      " [79]\n",
      " [79]\n",
      " [83]\n",
      " [60]\n",
      " [62]\n",
      " [67]\n",
      " [66]\n",
      " [73]\n",
      " [69]\n",
      " [73]\n",
      " [72]\n",
      " [74]\n",
      " [73]\n",
      " [76]\n",
      " [75]\n",
      " [74]\n",
      " [79]\n",
      " [80]\n",
      " [80]\n",
      " [59]\n",
      " [60]\n",
      " [64]\n",
      " [66]\n",
      " [70]\n",
      " [70]\n",
      " [71]\n",
      " [73]\n",
      " [71]\n",
      " [73]\n",
      " [72]\n",
      " [75]\n",
      " [77]\n",
      " [82]\n",
      " [78]\n",
      " [74]\n",
      " [52]\n",
      " [54]\n",
      " [58]\n",
      " [62]\n",
      " [63]\n",
      " [66]\n",
      " [68]\n",
      " [70]\n",
      " [68]\n",
      " [70]\n",
      " [70]\n",
      " [73]\n",
      " [74]\n",
      " [75]\n",
      " [77]\n",
      " [78]]\n",
      "0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-\n",
      "(96,)\n",
      "(224, 1)\n",
      "[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "  69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "  72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "  41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "  92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "  80  81  81  83  82  80  70  71  72  72  76  76  77  77  74  74  71  76\n",
      "  81  79  80  80 167 163 157 151 149 145 139 136 130 127 122 120 120 120\n",
      " 115 117  36  40  44  47  50  53  58  57  57  59  61  63  67  69  71  69\n",
      "  50  56  55  57  62  64  64  67  64  67  68  65  71  75  73  75  50  52\n",
      "  52  56  62  64  65  66  65  66  69  68  73  73  73  74  43  49  50  54\n",
      "  57  58  61  64  61  65  67  66  69  70  71  74  79  75  73  77  80  81\n",
      "  76  82  80  79  81  82  85  84  84  83  79  79  79  81  82  82  83  85\n",
      "  81  81  81  83  86  87  87  85]\n",
      "[ 91  93  85  90  95  93  86  91  87  87  87  86  88  87  90  87  70  73\n",
      "  69  71  76  76  77  78  74  75  76  78  79  80  78  81  65  64  68  67\n",
      "  72  71  74  73  73  70  72  74  77  77  78  78  13  19  24  29  35  37\n",
      "  41  47  44  48  51  55  58  59  59  62 102 100 102  97  99  99  97  96\n",
      "  92  91  89  89  91  91  90  88  76  78  76  77  79  80  81  81  79  81\n",
      "  80  81  81  83  82  80]\n",
      "정확도 계산중... \n",
      " 정확도는 다음과 같다 \n",
      "0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# 데이터셋 로드하기\n",
    "# [[your code]\n",
    "data = rows\n",
    "data2 = rows2\n",
    "data9 = rows9\n",
    "# 훈련용 데이터셋 나누기\n",
    "# [[your code]\n",
    "X_train = rows[:14]\n",
    "X_test = rows[14:]\n",
    "\n",
    "y_train = data[:14]\n",
    "y_test = data[14:]\n",
    "\n",
    "print(\"y_train\")\n",
    "print(y_train)\n",
    "print('-'*90)\n",
    "print('-'*90)\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "# y_test2 = data2[:16]\n",
    "# y_test2 = y_test2.reshape(16,4)\n",
    "# 훈련하기\n",
    "# [[your code]\n",
    "print('-/'*90)\n",
    "print('-/'*90)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "# X2, y2 = make_classification(n_samples=1000, n_features=4,\n",
    "#                              n_informative=2, n_redundant=0,\n",
    "#                              random_state=0, shuffle=False)\n",
    "# clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# clf.fit(X2, y2)\n",
    "# print(X2)\n",
    "# print('-/'*90)\n",
    "# print('-/'*90)\n",
    "xtt = np.array(X_train)\n",
    "xt = xtt.reshape(-1,)\n",
    "ytt = np.array(y_train) #numpy array 로 바꿈!!\n",
    "yt = ytt.reshape(-1,) # numpy array 차원 축소!!! 1차원으로 만들어주기\n",
    "ett = np.array(y_test)\n",
    "ettt = ett.reshape(-1,)\n",
    "\n",
    "print(\"xt : X_train 에 해당\")\n",
    "print(xt)\n",
    "print('-'*90)\n",
    "print(\"yt: y_train 에 해당 \")\n",
    "print(yt)\n",
    "print('-'*90)\n",
    "print(\"ett : y_test 에 해당\")\n",
    "print(ettt)\n",
    "print('-'*90)\n",
    "\n",
    "xt = np.expand_dims(xt, axis=0) # 차원 확대\n",
    "yt = np.expand_dims(yt, axis=0) # 차원 확대 \n",
    "np.expand_dims(xt, axis=0)\n",
    "np.expand_dims(yt, axis=0)\n",
    "\n",
    "print(\"xt\")\n",
    "print(xt)\n",
    "print('-'*90)\n",
    "print(\"yt\")\n",
    "print(yt)\n",
    "print('-'*90)\n",
    "print('-*'*90)\n",
    "# clf = MultiOutputClassifier(KNeighborsClassifier()).fit(xt,yt) # 2차원 이상의 배열이 필요함...\n",
    "knn = KNeighborsClassifier(n_neighbors=1)#여기에서 n_neighbers =1 이 중요!!! 입력 데이터 형태를 보면 1로 해야함\n",
    "knn = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=1)).fit(xt,yt) #2차원 필요\n",
    "print(\"knn 예측치\")\n",
    "knnmaster1 = knn.predict(xt[-2:]) #We select the training set with the [:-1] Python syntax,\n",
    "#which produces a new array that contains all but the last item from digits.data:\n",
    "print(knnmaster1)\n",
    "print('-*'*90)\n",
    "\n",
    "\n",
    "print(\"정답률=\", knn.score(y_pred2, xt)) ###########################\n",
    "\n",
    "\n",
    "print('-*'*90)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(xt, yt)\n",
    "print('-'*90)\n",
    "# 예측하기\n",
    "# [[your code]\n",
    "y_pred1 = model.predict(xt)\n",
    "# 정답률 출력하기\n",
    "# [[your code]\n",
    "print(y_pred1)\n",
    "print('-'*90)\n",
    "decision_tree = DecisionTreeClassifier(random_state=1)\n",
    "decision_tree.fit(xt, yt)\n",
    "y_pred2 = decision_tree.predict(xt)\n",
    "print(\"Decision Tree classifier 예측치\")\n",
    "print(y_pred2)\n",
    "print('-'*90)\n",
    "\n",
    "y_testt = np.transpose(y_test)\n",
    "print(y_test)\n",
    "print('-'*90)\n",
    "#y_test = np.expand_dims(y_test, axis=0)\n",
    "\n",
    "print('y_test')\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "print('-'*90)\n",
    "print('y_pred1')\n",
    "print(y_pred1)\n",
    "print('-'*90)\n",
    "print('-'*90)\n",
    "print(rows4)\n",
    "\n",
    "    \n",
    "    \n",
    "#svm 은 y 가 1차원이어야 한다고 한다.\n",
    "\n",
    "yt2 = yt.reshape(-1,) # numpy array 차원 축소!!! 1차원으로 만들어주기\n",
    "xt2 = xt.reshape(-1,)\n",
    "xt3 = xt2.reshape(-1, 1)\n",
    "yt3 = yt2.reshape(-1, 1)\n",
    "                  \n",
    "ett2 = ett.reshape(-1,)\n",
    "ett3 = ett.reshape(-1, 1)\n",
    "print(xt2)\n",
    "print(',=,='*90)\n",
    "print(\"xt3\")\n",
    "print(xt3)\n",
    "print('/./.'*90)\n",
    "from sklearn import svm\n",
    "#먼저 y_pred 의 shape 은?\n",
    "# for i in y_pred1[1:]:\n",
    "#     rows4.append(list(map(int,i.split(\",\"))))\n",
    "# rows4 = pd.DataFrame(y_pred1)\n",
    "# rows6 = pd.DataFrame(y_pred2)\n",
    "# rows5 = pd.DataFrame(y_test)\n",
    "# rows7=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "# print(rows4) #y예측치\n",
    "# print('-'*90)\n",
    "# print('rows5')\n",
    "# print(rows5) #y test\n",
    "clf = svm.SVC()\n",
    "print('-'*90)\n",
    "#roww1 = rows5.reshape(-1,) #dataframe 오류 \n",
    "#roww1 = y_test.reshape(-1,) # list 는 reshape 이 없다... np.array 로 바꿔주기\n",
    "# roww1 = np.array(y_test)\n",
    "# roww2 = roww1.reshape(-1,)\n",
    "# roww3 = pd.DataFrame(roww2)\n",
    "# roww4 = y_pred1.reshape(-1,)\n",
    "# print('roww2')\n",
    "# print(roww2)\n",
    "\n",
    "print('-'*90)\n",
    "print('-'*90)\n",
    "#print(\"정답률=\", clf.score(ett, yt3))\n",
    "print(ett3)\n",
    "ett4 =ett3.reshape(-1,)\n",
    "yt33 = yt3.reshape(-1,) #차원축소\n",
    "xt33 = xt3.reshape(-1,)\n",
    "clf.fit(xt3,yt3) #2차원으로 fit\n",
    "print('0-0-'*90)\n",
    "print(ett4.shape) \n",
    "print(yt3.shape)\n",
    "yt4 = yt3.reshape(-1,)\n",
    "print(yt4)\n",
    "yt5 = yt4[:96]\n",
    "print(yt5)\n",
    "accuracy = accuracy_score(ett4, yt5) #같은 shape 이어야 함.\n",
    "\n",
    "\n",
    "print(\"정확도 계산중... \")\n",
    "print(\" 정확도는 다음과 같다 \")\n",
    "print(accuracy)\n",
    "# from sklearn.datasets import make_blobs\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# X,y = make_multilabel_classification(n_classes= 3,random_state = 0)\n",
    "\n",
    "# clf.predict(X[-2:])\n",
    "\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# classifier = MultiOutputClassifier(knn, n_jobs=-1)\n",
    "# classifier.fit(y_test,y_pred)\n",
    "# predictions = classifier.predict(y_train)\n",
    "# classifier.score(y_true,np.array(y_train))\n",
    "\n",
    "# from sklearn.linear_model import RidgeClassifierCV\n",
    "# clf = RidgeClassifierCV().fit(X_train, y_train)\n",
    "# y_score = clf.decision_function(X_train)\n",
    "# roc_auc_score(y_test, y_train, average=None)\n",
    "\n",
    "# from sklearn.datasets import make_multilabel_classification\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# X, y = make_multilabel_classification(random_state=0)\n",
    "# inner_clf = LogisticRegression(solver=\"liblinear\", random_state=0)\n",
    "# clf = MultiOutputClassifier(inner_clf).fit(X, y)\n",
    "# y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])\n",
    "# roc_auc_score(y, y_score, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16d9f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import RidgeClassifierCV\n",
    "# clf = RidgeClassifierCV().fit(X_train, y_train)\n",
    "\n",
    "# roc_auc_score(rows4, rows5, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac8374",
   "metadata": {},
   "source": [
    "\n",
    "#열이 16개인 데이터셋\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8f9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
